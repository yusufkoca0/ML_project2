{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a432c4e",
   "metadata": {},
   "source": [
    "# THIS PROJECT HAD DONE BY :\n",
    "# MUSTAFA EMÄ°R PEKER               NO:2200356011\n",
    "# YUSUF KOCA                                NO:2200356013"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f1d04f",
   "metadata": {},
   "source": [
    "# BBM409 FALL 2022 ASSIGNMENT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeef757f",
   "metadata": {},
   "source": [
    "For this project we run our code in main() function. Also the pruning function is in the same class with the other functions because of that we can not split pruning function into a different cell in notebook to give seperate explanation.\n",
    "\n",
    "We explained our code before our code blocks. All of our report's content given as explaination above our code blocks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777443a8",
   "metadata": {},
   "source": [
    "Libraries that we imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a46756b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceca7671",
   "metadata": {},
   "source": [
    "Global variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6dd49165",
   "metadata": {},
   "outputs": [],
   "source": [
    "global total_entropy\n",
    "global data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e91fa2",
   "metadata": {},
   "source": [
    "is_discrete: This function determines that wheter a column is discrete or not. It determines the discreteness of a column by looking its unique values. If the unique values have more than 10 members and all of its members are not  string than it assumes that the column is discrete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a0b16de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#is it discrete or continuous\n",
    "def is_discrete(data, col):\n",
    "    unique = np.unique(data[:, col])\n",
    "    # if there are more than 10 unique values and these values are not strings then it is continuous\n",
    "    if len(unique) > 10 and not isinstance(unique[0], str):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e0d839",
   "metadata": {},
   "source": [
    "make_it_discrete_from_cont: This function takes a column and makes the column's values discrete by getting the max and min of the column. After that it calculates range and step of the column. Then it compares column's according row value with the intervals. After comparing it converts the value according to intervals. We chose to divide the range by 10. First we tried dividing by 2 , it performed better than when we chose to use all continous as attribute values. With some try and error we decided that dividing by 10 performs the best for the first part. For the pruning part choosing even number of attributes might be a problem. Because when we are taking majority of the attribute values there can be a tie between values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "7a5e1ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_it_discrete_from_cont(data, col):\n",
    "\n",
    "    # make countinous data discrete by taking the min and max of the column divide it into 10 equal parts\n",
    "    min = np.min(data[:, col])\n",
    "    max = np.max(data[:, col])\n",
    "    # get the range\n",
    "    range1 = max - min\n",
    "    # get the step\n",
    "    step = range1 / 10\n",
    "    # loop through the column\n",
    "    for i in range(len(data[:, col])):\n",
    "        # if the value is less than the mean replace it with 0 else replace it with 1\n",
    "        if data[i, col] < min + step:\n",
    "            data[i, col] = 0\n",
    "        elif data[i, col] < min + 2 * step:\n",
    "            data[i, col] = 1\n",
    "        elif data[i, col] < min + 3 * step:\n",
    "            data[i, col] = 2\n",
    "        elif data[i, col] < min + 4 * step:\n",
    "            data[i, col] = 3\n",
    "        elif data[i, col] < min + 5 * step:\n",
    "            data[i, col] = 4\n",
    "        elif data[i, col] < min + 6 * step:\n",
    "            data[i, col] = 5\n",
    "        elif data[i, col] < min + 7 * step:\n",
    "            data[i, col] = 6\n",
    "        elif data[i, col] < min + 8 * step:\n",
    "            data[i, col] = 7\n",
    "        elif data[i, col] < min + 9 * step:\n",
    "            data[i, col] = 8\n",
    "        else:\n",
    "            data[i, col] = 9\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3069f3c",
   "metadata": {},
   "source": [
    "make_all_discrete: This function combines make_it_discrete_from_cont and is_discrete. It takes whole data set and makes all columns discrete that are suitable for discretization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0209efaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_all_discrete(data):\n",
    "    # loop through the columns\n",
    "    for i in range(len(data[0]) - 1):\n",
    "        # check if the column is continous\n",
    "        if (is_discrete(data, i) == False):\n",
    "            # make the column discrete\n",
    "            if (isinstance(data[0][i], str) == False):\n",
    "                data = make_it_discrete_from_cont(data, i)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73a159e",
   "metadata": {},
   "source": [
    " remove_attrition:For simplicity reasons we changed the position of the 'Attrition' column to the end. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "96122d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_attrition(data):\n",
    "    # get the column indexed as 1\n",
    "    attrition = data[:, 1]\n",
    "    # remove the column indexed as 1\n",
    "    data = np.delete(data, 1, 1)\n",
    "    # append the column indexed as 1 to the end of the data set\n",
    "    data = np.append(data, attrition[:, None], axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dd14b7",
   "metadata": {},
   "source": [
    "shuffle: A function to shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a282b29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle function from scratch\n",
    "def shuffle(aux, n):\n",
    "    # shuffle\n",
    "    for i in range(n):\n",
    "        # generate random index\n",
    "        index = np.random.randint(0, len(aux))\n",
    "        # swap\n",
    "        aux[i], aux[index] = aux[index], aux[i]\n",
    "\n",
    "    return aux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f103a260",
   "metadata": {},
   "source": [
    "split_data: This function splits the data according to 5cv. After splitting it returns 5 training and 5 test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "227b120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into 5 cross validation train and test sets\n",
    "def split_data(data, k):\n",
    "    shuffle(data, len(data))\n",
    "    split = np.array_split(data, k)\n",
    "    # concatenate the split data\n",
    "    training_set1 = np.concatenate((split[0], split[1], split[2], split[3]), axis=0)\n",
    "    test_set1 = split[4]\n",
    "\n",
    "    training_set2 = np.concatenate((split[0], split[1], split[2], split[4]), axis=0)\n",
    "    test_set2 = split[3]\n",
    "\n",
    "    training_set3 = np.concatenate((split[0], split[1], split[3], split[4]), axis=0)\n",
    "    test_set3 = split[2]\n",
    "\n",
    "    training_set4 = np.concatenate((split[0], split[2], split[3], split[4]), axis=0)\n",
    "    test_set4 = split[1]\n",
    "\n",
    "    training_set5 = np.concatenate((split[1], split[2], split[3], split[4]), axis=0)\n",
    "    test_set5 = split[0]\n",
    "\n",
    "    return [training_set1, test_set1], [training_set2, test_set2], [training_set3, test_set3], [training_set4,\n",
    "                                                                                                test_set4], [training_set5, test_set5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cca48e5",
   "metadata": {},
   "source": [
    "total_entropy: This function calculates total entropy of a given set. This function is used to calculate information gain ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "fbe35fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate entropy of a given data set numpy array with the last index being the attrition column with Yes or No\n",
    "def total_entropy(data):\n",
    "    # get the number of rows in the data set\n",
    "    n = len(data)\n",
    "\n",
    "    # get the number of rows where attrition is yes\n",
    "    yes = len(data[data[:, -1] == 'Yes'])\n",
    "    # get the number of rows where attrition is no\n",
    "    no = len(data[data[:, -1] == 'No'])\n",
    "    # calculate the entropy\n",
    "    if yes == 0 or no == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -((yes / n) * np.log2(yes / n) + (no / n) * np.log2(no / n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e38c4c",
   "metadata": {},
   "source": [
    "unique_values: This function takes a column and finds the unique values in the column. After that it calculates the entropy of the given column's unique values(i.e. it calculates the entropy of an attribute's unique values).After that it returns a dictionary where unique values are the keys and their entropy values are the... values:))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ba46bf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_values(data, col):\n",
    "    # get the unique values in the column\n",
    "    unique = np.unique(data[:, col])\n",
    "    # create a dictionary to store the unique values and their corresponding entropy\n",
    "    unique_dict = {}\n",
    "    # loop through the unique values\n",
    "    for i in unique:\n",
    "        # get the number of rows where the unique value is present\n",
    "        n = len(data[data[:, col] == i])\n",
    "        # get the number of rows where the unique value is present and attrition is yes\n",
    "        yes = len(data[(data[:, col] == i) & (data[:, -1] == 'Yes')])\n",
    "        # get the number of rows where the unique value is present and attrition is no\n",
    "        no = len(data[(data[:, col] == i) & (data[:, -1] == 'No')])\n",
    "        # calculate the entropy\n",
    "        if yes == 0 or no == 0:\n",
    "            unique_dict[i] = 0\n",
    "        else:\n",
    "            unique_dict[i] = -((yes / n) * np.log2(yes / n) + (no / n) * np.log2(no / n))\n",
    "    return unique_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2713481e",
   "metadata": {},
   "source": [
    "attribute_vars : This function returns the unique values of a given column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "27bc55c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_vars(data, col):\n",
    "    unique = np.unique(data[:, col])\n",
    "    # get keys in an array\n",
    "    keys = []\n",
    "    for i in unique:\n",
    "        keys.append(i)\n",
    "    return keys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd36d9d",
   "metadata": {},
   "source": [
    "information_gain: This function calculates information gain of given column using previously defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b64e4807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(data, col):\n",
    "    # get the total entropy of the data set\n",
    "    total = total_entropy(data)\n",
    "    # get the unique values and their corresponding entropy\n",
    "    unique_dict = unique_values(data, col)\n",
    "    # get the number of rows in the data set\n",
    "    n = len(data)\n",
    "    # calculate the information gain\n",
    "    for key, value in unique_dict.items():\n",
    "        unique_dict[key] = (len(data[data[:, col] == key]) / n) * value\n",
    "    return total - sum(unique_dict.values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a833a5fe",
   "metadata": {},
   "source": [
    "info_gain_max: This function takes a numpy array. It runs through all the columns of the numpy array. It calculates each column's information gain using information_gain function. After that it returns most informative attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "fdf741f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate all info gains and return the column index with the highest info gain\n",
    "def info_gain_max(data):\n",
    "    # create a dictionary to store the column index and its corresponding info gain\n",
    "    info_gain_dict = {}\n",
    "    # loop through the columns\n",
    "    for i in range(len(data[0]) - 1):\n",
    "        # calculate the info gain of the column\n",
    "        info_gain_dict[i] = information_gain(data, i)\n",
    "    # return the column index with the highest info gain\n",
    "\n",
    "    return max(info_gain_dict, key=info_gain_dict.get)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db2f742",
   "metadata": {},
   "source": [
    "getNameByIndex: This function takes an index and returns initial attribute's name.\n",
    "getIndexByName: This function takes a name and returns the index of a given name according to initial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a144ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNameByIndex(index):\n",
    "    list = ['Age', 'BusinessTravel', 'DailyRate', 'Department', 'DistanceFromHome', 'Education', 'EducationField', 'EmployeeCount', 'EmployeeNumber', 'EnvironmentSatisfaction', 'Gender', 'HourlyRate', 'JobInvolvement', 'JobLevel', 'JobRole', 'JobSatisfaction', 'MaritalStatus', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'Over18', 'OverTime', 'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction', 'StandardHours', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager', 'Attrition']\n",
    "    return list[index]\n",
    "\n",
    "def getIndexByName(name):\n",
    "    list = ['Age', 'BusinessTravel', 'DailyRate', 'Department', 'DistanceFromHome', 'Education', 'EducationField', 'EmployeeCount', 'EmployeeNumber', 'EnvironmentSatisfaction', 'Gender', 'HourlyRate', 'JobInvolvement', 'JobLevel', 'JobRole', 'JobSatisfaction', 'MaritalStatus', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'Over18', 'OverTime', 'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction', 'StandardHours', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager', 'Attrition']\n",
    "    return list.index(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f07af1",
   "metadata": {},
   "source": [
    "We have two types of nodes. For that we created two classes. \n",
    "AttributeNode class : This is the main node for the our tree it holds attributes and its childs.\n",
    "LeafNode class: This is the leaf node that holds the value for the desicion. It holds an attribute that indicates its parent node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "96cb2c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a node class\n",
    "class AttributeNode:\n",
    "    def __init__(self, attribute,children):\n",
    "        self.attribute = attribute\n",
    "        self.children = children\n",
    "\n",
    "\n",
    "\n",
    "class LeafNode:\n",
    "    def __init__(self, value, attribute):\n",
    "        self.value = value\n",
    "        self.attribute = attribute\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9d8d34",
   "metadata": {},
   "source": [
    "Here we have the DecisionTree class. This class has all the functions that needed for building a tree, making prediction out of a tree, pruning a tree and calculating the classification matrix.\n",
    "\n",
    "build_tree: This function takes a dataset to build a tree.\n",
    "            1-) It takes the most informative column and calculates its unique values.\n",
    "            2-) Loops through its unique values and takes the matching rows with its current unique value.\n",
    "            3-) After that it deletes the firstly calculated most informative column from dataset.\n",
    "            4-) After deletion it checks if the dataset is pure(fully informative)\n",
    "            5-) If it is pure it creates a leaf node that has the value of the fully informative dataset's \n",
    "                attrition value. And appends the leaf to the tree.\n",
    "            6-) If it is not pure then function calls itself with the not-pure dataset and goes back to the first\n",
    "                step. After all the recursive calls it creates a node and appends to the tree.\n",
    "            7-) After looping throuh unique values it creates a node with the highest informative attribute and it \n",
    "                appends node to the tree.\n",
    "            8-) After calculating all the nodes in the tree it returns the root node of the tree.\n",
    "\n",
    "predict: This function takes a row and it guesses the attrition value of the row by roaming the according to row's \n",
    "         attribute values. It returns the guessed attrition value.\n",
    "\n",
    "accuracy: This function takes a test set and compares its row's attrition values with the predicted values. After             that it calculates the metrices that are needed for evaluating the tree's success and returns them.\n",
    "\n",
    "find_twigs: This function finds all the nodes whose children are all leafs.It does that roaming through allmost all             nodes and checking their children. It returns a list of twigs.\n",
    "\n",
    "find_parent: This function checks all the nodes by recursively . If it finds the given node it understands that the              previous node that called the current recursion is the parent. After that it returns the parent node.\n",
    "\n",
    "get_majority: This function takes a twig node and it calculates the majority value among its children. If the\n",
    "              values are equal in numbers then it returns the first value it encountered with. Otherwise it returns\n",
    "              the majority value.\n",
    "             \n",
    "entropy: This function calculates the entropy of a given twig.\n",
    "              \n",
    "information_gain: This function calculates the information gain of a given twig using the entropy function.\n",
    "\n",
    "prune: This function prunes the tree by following these steps:\n",
    "        Step 1: Catalog all twigs in the tree\n",
    "        Step 2: Find the twig with the least Information Gain\n",
    "        Step 3: Remove all child nodes of the twig\n",
    "        Step 4: Relabel twig as a leaf (Set the majority of âPositiveâ or âNegativeâ as leaf value)\n",
    "        Step 5: Measure the accuracy value of your decision tree model with removed twig on the validation set \n",
    "        (\"Current Accuracy\")\n",
    "        If âCurrent Accuracy â¥ Last Accuracyâ : Jump to âStep1â\n",
    "        Else : Revert the last changes done in Step 3,4 and then terminate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "cb1eb970",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "\n",
    "    def build_tree(self, data):\n",
    "        # get the column index with the highest info gain\n",
    "        max_info_gain = info_gain_max(data)\n",
    "        # get the unique values in the column\n",
    "        unique = attribute_vars(data, max_info_gain)\n",
    "        # create a list to store the children nodes\n",
    "        children = []\n",
    "        # loop through the unique values\n",
    "        for i in unique:\n",
    "            # get the rows where the unique value is present\n",
    "            rows = data[data[:, max_info_gain] == i]\n",
    "            # remove the column with the highest info gain\n",
    "            rows = np.delete(rows, max_info_gain, 1)\n",
    "            # check if the rows are pure\n",
    "            if (is_pure(rows)):\n",
    "                # create a leaf node\n",
    "                leaf = LeafNode(rows[0][-1], i)\n",
    "                # append the leaf node to the children list\n",
    "\n",
    "                children.append(leaf)\n",
    "            else:\n",
    "                # create a new attribute node\n",
    "                node = AttributeNode(i, self.build_tree(rows))\n",
    "                # append the node to the children list\n",
    "                children.append(node)\n",
    "        # create an attribute node\n",
    "        node = AttributeNode(getNameByIndex(max_info_gain), children)\n",
    "        return node\n",
    "\n",
    "\n",
    "    # make a method that takes a row and returns the label\n",
    "    def predict(self, row, node):\n",
    "        # check if the node is a leaf node\n",
    "        if (isinstance(node, LeafNode)):\n",
    "            return node.value\n",
    "        else:\n",
    "            # get the attribute\n",
    "            attribute = node.attribute\n",
    "            # loop through the children\n",
    "\n",
    "            if (isinstance(node.children, AttributeNode)):\n",
    "                # take the node.children's children\n",
    "                children = node.children.children\n",
    "                node_attribute = node.children.attribute\n",
    "\n",
    "                node1 = AttributeNode(node_attribute, children)\n",
    "                node = node1\n",
    "                attribute = node.attribute\n",
    "\n",
    "\n",
    "            for i in node.children:\n",
    "                # check if the attribute is equal to the child's attribute\n",
    "                # print(row[getIndexByName(attribute)])\n",
    "                if (row[getIndexByName(attribute)] == i.attribute):\n",
    "                    # return the label\n",
    "                    return self.predict(row, i)\n",
    "\n",
    "\n",
    "\n",
    "    def accuracy(self, data, node):\n",
    "        # create a list to store the predictions\n",
    "        predictions = []\n",
    "        tp = 0\n",
    "        tn = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "\n",
    "        # loop through the data\n",
    "        for i in data:\n",
    "            # get the prediction\n",
    "            prediction = self.predict(i, node)\n",
    "            # append the prediction to the list\n",
    "            predictions.append(prediction)\n",
    "        # get the number of correct predictions\n",
    "        correct = 0\n",
    "        # loop through the predictions\n",
    "        for i in range(len(predictions)):\n",
    "            # check if the prediction is correct\n",
    "            if (predictions[i] == data[i][-1]):\n",
    "                # increment the correct predictions\n",
    "                correct += 1\n",
    "            if ( predictions[i] == 'Yes' and data[i][-1] == 'Yes'):\n",
    "                tp += 1\n",
    "            if ( predictions[i] == 'No' and data[i][-1] == 'No'):\n",
    "                tn += 1\n",
    "            if ( predictions[i] == 'Yes' and data[i][-1] == 'No'):\n",
    "                fp += 1\n",
    "            if ( predictions[i] == 'No' and data[i][-1] == 'Yes'):\n",
    "                fn += 1\n",
    "\n",
    "\n",
    "        # calculate the accuracy\n",
    "        accuracy = correct / len(predictions)\n",
    "\n",
    "        # calculate the precision\n",
    "        precision = tp / (tp + fp)\n",
    "\n",
    "        # calculate the recall\n",
    "        recall = tp / (tp + fn)\n",
    "\n",
    "        # calculate the f1 score\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        # return all the values\n",
    "        return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "    # find all the twigs in the tree. The twigs are the nodes whose children are all leaves.\n",
    "    def find_twigs(self, node):\n",
    "        # create a list to store the twigs\n",
    "        twigs = []\n",
    "        # check if the node is a leaf node\n",
    "        if (isinstance(node, LeafNode)):\n",
    "            return twigs\n",
    "        # check if the node's children are all leaf nodes\n",
    "        if (isinstance(node.children, AttributeNode)):\n",
    "            # take the node.children's children\n",
    "            children = node.children.children\n",
    "            node_attribute = node.children.attribute\n",
    "\n",
    "            node1 = AttributeNode(node_attribute, children)\n",
    "            node = node1\n",
    "\n",
    "        if (all(isinstance(i, LeafNode) for i in node.children)):\n",
    "            # append the node to the twigs list\n",
    "            twigs.append(node)\n",
    "        else:\n",
    "            # loop through the children\n",
    "            for i in node.children:\n",
    "                # get the twigs from the child\n",
    "                twig = self.find_twigs(i)\n",
    "                # append the twig to the twigs list\n",
    "                twigs.extend(twig)\n",
    "        return twigs\n",
    "\n",
    "\n",
    "    # find parent of a twig\n",
    "    def find_parent(self, node, twig):\n",
    "        # check if the node is a leaf node\n",
    "\n",
    "        if (isinstance(node, LeafNode)):\n",
    "            return None\n",
    "        # check if the node's children are all leaf nodes\n",
    "        if (isinstance(node.children, AttributeNode)):\n",
    "            # take the node.children's children\n",
    "            children = node.children.children\n",
    "            node_attribute = node.children.attribute\n",
    "\n",
    "            node1 = AttributeNode(node_attribute, children)\n",
    "            node = node1\n",
    "\n",
    "        if (all(isinstance(i, LeafNode) for i in node.children)):\n",
    "            # compare the node' attribute with the twig's attribute and compare the node's children with the twig's children\n",
    "            if (node.attribute == twig.attribute):\n",
    "                if (all(node.children[i].attribute == twig.children[i].attribute for i in range(len(node.children)))):\n",
    "                    return node\n",
    "            return None\n",
    "        else:\n",
    "            # loop through the children\n",
    "            for i in node.children:\n",
    "                # check if the child is the twig\n",
    "\n",
    "                if (i == twig):\n",
    "                    # return the node\n",
    "                    return node\n",
    "                # get the parent from the child\n",
    "                parent = self.find_parent(i, twig)\n",
    "                # check if the parent is not None\n",
    "                if (parent is not None):\n",
    "\n",
    "                    return parent\n",
    "\n",
    "\n",
    "    def get_majority(self, node):\n",
    "        # create a list to store the labels\n",
    "        labels = []\n",
    "        # loop through the children\n",
    "        for i in node.children:\n",
    "            # append the label to the list\n",
    "            labels.append(i.value)\n",
    "        # get the most common label\n",
    "\n",
    "        # if there are more than one label with the same frequency, return the first one\n",
    "        if (labels.count(max(set(labels), key=labels.count)) > 1):\n",
    "            return labels[0]\n",
    "\n",
    "\n",
    "    def entropy(self, positive, negative):\n",
    "        # calculate the entropy\n",
    "        total = positive + negative\n",
    "        entropy = 0\n",
    "        if (positive != 0):\n",
    "            entropy += -(positive / total) * math.log2(positive / total)\n",
    "        if (negative != 0):\n",
    "            entropy += -(negative / total) * math.log2(negative / total)\n",
    "        return entropy\n",
    "\n",
    "    def information_gain(self, node):\n",
    "        # get the number of positive and negative labels\n",
    "        positive = 0\n",
    "        negative = 0\n",
    "        # loop through the children\n",
    "        for i in node.children:\n",
    "            # check if the label is positive\n",
    "            if (i.value == \"Yes\"):\n",
    "                # increment the positive count\n",
    "                positive += 1\n",
    "            else:\n",
    "                # increment the negative count\n",
    "                negative += 1\n",
    "        # calculate the information gain\n",
    "        info_gain = 1 - self.entropy(positive, negative)\n",
    "        return info_gain\n",
    "\n",
    "\n",
    "    def prune(self, node, validation_data, last_accuracy):\n",
    "        # find all the twigs in the tree\n",
    "        twigs = self.find_twigs(node)\n",
    "        # check if the twigs list is empty\n",
    "        if (len(twigs) == 0):\n",
    "            return node\n",
    "        # find the twig with the least information gain\n",
    "        twig = min(twigs, key=lambda x: self.information_gain(x))\n",
    "        # find the parent of the twig\n",
    "        parent = self.find_parent(node, twig)\n",
    "        # remove all the children of the twig\n",
    "        majority = self.get_majority(twig)\n",
    "        twig.children = []\n",
    "        # relabel the twig as a leaf\n",
    "        twig.children.append(LeafNode(majority, twig.attribute))\n",
    "        # measure the accuracy of the tree\n",
    "        current_accuracy = self.accuracy(validation_data, node)[0]\n",
    "        # check if the current accuracy is greater than or equal to the last accuracy\n",
    "        if (current_accuracy > last_accuracy):\n",
    "            # prune the tree\n",
    "            print(\"Pruning : \" + str(current_accuracy))\n",
    "            return self.prune(node, validation_data, current_accuracy)\n",
    "        else:\n",
    "            # revert the changes\n",
    "            twig.children = parent.children\n",
    "            return node\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b468f9",
   "metadata": {},
   "source": [
    "is_pure: This method checks if a given set is full informative or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "341c9833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is pure method\n",
    "def is_pure(data):\n",
    "    # get the number of rows in the data set\n",
    "    n = len(data)\n",
    "    # get the number of rows where attrition is yes\n",
    "    yes = len(data[data[:, -1] == 'Yes'])\n",
    "    # get the number of rows where attrition is no\n",
    "    no = len(data[data[:, -1] == 'No'])\n",
    "    # check if the data set is pure\n",
    "    if yes == n or no == n:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bd31e9",
   "metadata": {},
   "source": [
    "And this is the main function that we run our code on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ab753e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create main method\n",
    "def main():\n",
    "    # read csv file and convert it into numpy array\n",
    "    df = pd.read_csv('WA_Fn-UseC_-HR-Employee-Attrition.csv')\n",
    "\n",
    "    names_of_columns = df.columns\n",
    "\n",
    "    names_of_columns = names_of_columns.tolist()\n",
    "\n",
    "    # remove the attrition column from the list of column names\n",
    "    names_of_columns.remove('Attrition')\n",
    "    # append the attrition column to the end of the list of column names\n",
    "    names_of_columns.append('Attrition')\n",
    "\n",
    "    data = df.values\n",
    "\n",
    "    data = remove_attrition(data)\n",
    "\n",
    "    data = make_all_discrete(data)\n",
    "\n",
    "    data = shuffle(data, len(data))\n",
    "\n",
    "    splitted_data = split_data(data,5)\n",
    "\n",
    "    avg_accuracy = 0.0\n",
    "    avg_precision = 0.0\n",
    "    avg_recall = 0.0\n",
    "    avg_f1 = 0.0\n",
    "\n",
    "\n",
    "    dt = DecisionTree()\n",
    "\n",
    "    for i in range(5):\n",
    "        train_data = splitted_data[i][0]\n",
    "        test_data = splitted_data[i][1]\n",
    "\n",
    "        node = dt.build_tree(train_data)\n",
    "        avg_accuracy += dt.accuracy(test_data, node)[0]\n",
    "        avg_precision += dt.accuracy(test_data, node)[1]\n",
    "        avg_recall += dt.accuracy(test_data, node)[2]\n",
    "        avg_f1 += dt.accuracy(test_data, node)[3]\n",
    "\n",
    "\n",
    "    avg_accuracy /= 5\n",
    "    avg_precision /= 5\n",
    "    avg_recall /= 5\n",
    "    avg_f1 /= 5\n",
    "\n",
    "\n",
    "    print(\"Average Accuracy: \", avg_accuracy)\n",
    "    print(\"Average Precision: \", avg_precision)\n",
    "    print(\"Average Recall: \", avg_recall)\n",
    "    print(\"Average F1: \", avg_f1)\n",
    "    \n",
    "    pruning_validation = data[0:294]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    dt.prune(node, pruning_validation, avg_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "55639f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy:  0.7149659863945578\n",
      "Average Precision:  0.7429158918229637\n",
      "Average Recall:  0.765911639188235\n",
      "Average F1:  0.7481856581135096\n",
      "Pruning : 0.7721088435374149\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920e9833",
   "metadata": {},
   "source": [
    "Here we average accuracy, precision, recall, f1 scores and accuracy score after pruning . We ran this code several times and we can say that our model has average accuracy between 70-90 percent. Usually after pruning our accuracy level increases between 10 percent.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
